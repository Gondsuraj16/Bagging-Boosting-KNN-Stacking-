{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Bagging & Boosting KNN & Stacking**"
      ],
      "metadata": {
        "id": "GYdZdotuvWAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 1: What is the fundamental idea behind ensemble techniques?**\n",
        "### **How does bagging differ from boosting in terms of approach and objective?**\n",
        "\n",
        "#### **Fundamental Idea Behind Ensemble Techniques**\n",
        "The fundamental idea of **ensemble techniques** is to combine multiple individual models to create a more powerful and accurate final model.  \n",
        "By aggregating the predictions of several weak learners, ensemble methods help to **reduce errors**, **improve accuracy**, and **increase model robustness**.\n",
        "\n",
        "Ensemble techniques mainly aim to:\n",
        "- **Reduce variance** (e.g., Bagging)\n",
        "- **Reduce bias** (e.g., Boosting)\n",
        "- **Improve overall model generalization**\n",
        "\n",
        "---\n",
        "\n",
        "#### **Bagging (Bootstrap Aggregating)**\n",
        "\n",
        "**Approach:**\n",
        "- Bagging trains multiple models **independently** on different random subsets of the training data using **bootstrapping** (sampling with replacement).\n",
        "- Final predictions are made by **averaging** (for regression) or **majority voting** (for classification).\n",
        "\n",
        "**Objective:**\n",
        "- To **reduce variance** and prevent overfitting by averaging out the errors of multiple models.\n",
        "\n",
        "**Example:**  \n",
        "Random Forest (an ensemble of Decision Trees using bagging).\n",
        "\n",
        "---\n",
        "\n",
        "#### **Boosting**\n",
        "\n",
        "**Approach:**\n",
        "- Boosting builds models **sequentially**, where each new model tries to **correct the errors** made by the previous ones.\n",
        "- It assigns **higher weights** to misclassified samples so that subsequent models focus more on difficult cases.\n",
        "\n",
        "**Objective:**\n",
        "- To **reduce bias** and convert weak learners into a strong one by iteratively improving performance.\n",
        "\n",
        "\n",
        "#### **Key Differences Between Bagging and Boosting**\n",
        "\n",
        "| **Aspect** | **Bagging** | **Boosting** |\n",
        "|-------------|-------------|--------------|\n",
        "| **Training Method** | Parallel (independent) | Sequential (dependent) |\n",
        "| **Main Goal** | Reduce variance | Reduce bias |\n",
        "| **Data Sampling** | Bootstrap sampling (with replacement) | Weighted sampling (focus on errors) |\n",
        "| **Model Weighting** | Equal weight | Weighted by performance |\n",
        "| **Overfitting Risk** | Low | High (if not regularized) |\n",
        "| **Examples** | Random Forest | AdaBoost, XGBoost, Gradient Boosting |"
      ],
      "metadata": {
        "id": "D1MjPHZ4vc0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 2: Explain how the Random Forest Classifier reduces overfitting compared to a single decision tree. Mention the role of two key hyperparameters in this process.**\n",
        "\n",
        "---\n",
        "\n",
        "#### **How Random Forest Reduces Overfitting**\n",
        "\n",
        "A **single Decision Tree** tends to overfit because it learns every pattern — even noise — from the training data.  \n",
        "The **Random Forest Classifier** overcomes this by building an **ensemble of many Decision Trees**, each trained on a random portion of the data and features.  \n",
        "The idea is that **multiple uncorrelated models combined together** will generalize better than any individual one.\n",
        "\n",
        "**Key Techniques Used:**\n",
        "1. **Bootstrap Sampling (Bagging):**  \n",
        "   - Each tree is trained on a random sample of the dataset (with replacement).  \n",
        "   - This ensures trees are diverse and not all dependent on the same samples.\n",
        "\n",
        "2. **Random Feature Selection:**  \n",
        "   - At every node split, a random subset of features is considered.  \n",
        "   - This prevents dominant features from biasing all trees and improves generalization.\n",
        "\n",
        "3. **Aggregation (Voting/Averaging):**  \n",
        "   - Predictions from all trees are combined using **majority voting** (for classification) or **averaging** (for regression).  \n",
        "   - This ensemble averaging smooths out noise and lowers variance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Role of Two Key Hyperparameters**\n",
        "\n",
        "1. ### **`n_estimators` (Number of Trees)**\n",
        "   - Controls how many Decision Trees are built in the forest.\n",
        "   - **Higher values** → more stable and accurate predictions (reduces variance).\n",
        "   - However, increasing it too much can increase training time.\n",
        "   - **Typical range:** 100–500 for most datasets.\n",
        "\n",
        "   ✅ *Effect on Overfitting:*  \n",
        "   A higher number of trees reduces the chance that the model overfits, since averaging many diverse trees smooths noisy predictions.\n",
        "\n",
        "---\n",
        "\n",
        "2. ### **`max_features` (Number of Features Considered per Split)**\n",
        "   - Controls how many features the model considers when splitting a node.\n",
        "   - Smaller values add **more randomness**, reducing correlation among trees.\n",
        "   - Common defaults:\n",
        "     - Classification → `max_features='sqrt'`\n",
        "     - Regression → `max_features='auto'` (all features)\n",
        "\n",
        "   ✅ *Effect on Overfitting:*  \n",
        "   Using fewer features per split reduces overfitting by forcing trees to explore different parts of the feature space instead of all using the same dominant features.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Summary Table**\n",
        "\n",
        "| **Aspect** | **Decision Tree** | **Random Forest** |\n",
        "|-------------|------------------|-------------------|\n",
        "| **Training Data** | Full dataset | Bootstrapped subsets |\n",
        "| **Feature Selection** | All features | Random subset per node |\n",
        "| **Model Correlation** | High | Low |\n",
        "| **Overfitting Tendency** | High | Low |\n",
        "| **Key Hyperparameters** | – | `n_estimators`, `max_feature"
      ],
      "metadata": {
        "id": "V0qmtawkxXUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 3: What is Stacking in Ensemble Learning?**\n",
        "---\n",
        "\n",
        "#### **Definition**\n",
        "**Stacking (Stacked Generalization)** is an **ensemble learning technique** that combines the predictions of multiple different models (base learners) using another model (called a **meta-learner** or **blender**) to produce the final prediction.\n",
        "\n",
        "Instead of simple averaging (as in Bagging) or sequential correction (as in Boosting), Stacking **learns how to best combine the outputs** of various models through another machine learning algorithm.\n",
        "\n",
        "\n",
        "#### **How Stacking Differs from Bagging and Boosting**\n",
        "\n",
        "| **Aspect** | **Bagging** | **Boosting** | **Stacking** |\n",
        "|-------------|--------------|--------------|---------------|\n",
        "| **Model Training** | Parallel | Sequential | Parallel (base) + one meta model |\n",
        "| **Goal** | Reduce variance | Reduce bias | Combine diverse models |\n",
        "| **Base Learners** | Usually same type (e.g., Decision Trees) | Usually same type (e.g., weak learners) | Different models (e.g., KNN, SVM, RF, LR) |\n",
        "| **Combining Method** | Averaging or Voting | Weighted combination (by performance) | Meta-learner learns how to combine predictions |\n",
        "| **Complexity** | Moderate | High | High (multi-level model) |\n",
        "| **Overfitting** | Less likely | Can overfit if not regularized | Can overfit if too complex |\n",
        "\n",
        "---\n",
        "\n",
        "#### **Example Use Case**\n",
        "\n",
        "**Use Case:** Predicting whether a customer will default on a loan.\n",
        "\n",
        "- **Base Models (Level 0):**\n",
        "  - Logistic Regression → captures linear relationships  \n",
        "  - Random Forest → captures non-linear interactions  \n",
        "  - KNN → works well on local patterns\n",
        "\n",
        "- **Meta Model (Level 1):**\n",
        "  - Logistic Regression → takes predictions from the above models as inputs and learns the optimal combination\n",
        "---\n"
      ],
      "metadata": {
        "id": "4G6UevVYxjqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 4: What is the OOB Score in Random Forest, and why is it useful?**\n",
        "### **How does it help in model evaluation without a separate validation set?**\n",
        "\n",
        "---\n",
        "\n",
        "#### **Definition**\n",
        "The **OOB Score (Out-of-Bag Score)** is an internal performance evaluation method used in the **Random Forest algorithm**.  \n",
        "When Random Forest builds each tree, it uses **bootstrap sampling**—that is, sampling the training data **with replacement**.  \n",
        "This means some samples are not used to train a particular tree; these unused samples are known as **Out-of-Bag (OOB) samples**.\n",
        "\n",
        "The OOB Score is computed by predicting these OOB samples using the trees that **did not see** them during training.  \n",
        "It provides an unbiased estimate of the model’s performance, similar to cross-validation, but **without needing a separate validation set**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **How the OOB Score Works**\n",
        "1. Each Decision Tree is trained on about **63% of the training data** (bootstrapped samples).  \n",
        "   The remaining **~37%** of samples are left out (OOB samples).\n",
        "\n",
        "2. Once all trees are trained, each OOB sample is passed only through the trees that did **not** include it in training.\n",
        "\n",
        "3. The final prediction for each OOB sample is obtained by **aggregating (voting or averaging)** predictions from those trees.\n",
        "\n",
        "4. The accuracy of these predictions is the **OOB Score**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Why the OOB Score is Useful**\n",
        "\n",
        "- ✅ **No Separate Valid**\n"
      ],
      "metadata": {
        "id": "rwQzn-FPyu5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 5: Compare AdaBoost and Gradient Boosting**\n",
        "\n",
        "---\n",
        "\n",
        "#### **Overview**\n",
        "Both **AdaBoost (Adaptive Boosting)** and **Gradient Boosting** are **boosting algorithms**, which combine multiple weak learners (usually shallow decision trees) sequentially to form a strong predictive model.  \n",
        "However, they differ in **how they handle errors**, **adjust weights**, and **optimize performance**.\n",
        "\n",
        "---\n",
        "\n",
        "### **1️⃣ How They Handle Errors from Weak Learners**\n",
        "\n",
        "| **Aspect** | **AdaBoost** | **Gradient Boosting** |\n",
        "|-------------|--------------|------------------------|\n",
        "| **Error Handling** | Focuses on **misclassified samples** by assigning them higher weights in the next iteration. | Focuses on **minimizing the loss function’s residuals** (difference between predicted and actual values). |\n",
        "| **Learning Mechanism** | Reweights samples: Misclassified → higher weight, Correct → lower weight. | Fits the next tree to the **residual errors** of the previous model. |\n",
        "| **Goal** | Correct previous classification mistakes by emphasizing hard examples. | Reduce overall prediction error by optimizing the gradient of the loss function. |\n",
        "\n",
        "---\n",
        "\n",
        "### **2️⃣ Weight Adjustment Mechanism**\n",
        "\n",
        "| **Aspect** | **AdaBoost** | **Gradient Boosting** |\n",
        "|-------------|--------------|------------------------|\n",
        "| **Weight Update** | Each sample has a weight. After each iteration, misclassified samples get higher weights so that the next learner focuses more on them. | Each tree is trained on the **residuals** (gradients) instead of reweighting samples. The next model tries to correct the remaining errors. |\n",
        "| **Model Weight (α)** | Each weak learner gets a weight based on its accuracy — better learners get higher importance. | Each new tree is scaled by a **learning rate (shrinkage parameter)** to control contribution and prevent overfitting. |\n",
        "| **Error Emphasis** | Explicit — directly increases weights for misclassified points. | Implicit — errors are reduced by following the gradient of the loss function. |\n",
        "\n",
        "---\n",
        "\n",
        "### **3️⃣ Typical Use Cases**\n",
        "\n",
        "| **Aspect** | **AdaBoost** | **Gradient Boosting** |\n",
        "|-------------|--------------|------------------------|\n",
        "| **Best For** | Clean, less noisy datasets where errors can be corrected iteratively. | Complex\n"
      ],
      "metadata": {
        "id": "C5soaD-Ny_9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 6: Why does CatBoost perform well on categorical features without requiring extensive preprocessing?**\n",
        "---\n",
        "\n",
        "#### **Overview**\n",
        "**CatBoost** (short for *Categorical Boosting*) is a gradient boosting algorithm developed by **Yandex**, designed specifically to handle **categorical data** efficiently.  \n",
        "Unlike other boosting algorithms such as XGBoost or LightGBM, which require manual preprocessing like **One-Hot Encoding** or **Label Encoding**, CatBoost can **natively process categorical variables** — making it more accurate and faster on such datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why CatBoost Performs Well on Categorical Features**\n",
        "\n",
        "CatBoost automatically converts categorical data into numerical representations using **statistical techniques** that capture relationships between category values and target labels.  \n",
        "This allows it to **retain useful information** without inflating the feature space (as in one-hot encoding).\n",
        "\n",
        "---\n",
        "\n",
        "### **How CatBoost Handles Categorical Variables**\n",
        "\n",
        "CatBoost introduces two innovative techniques to process categorical data effectively:\n",
        "\n",
        "### **Example in Code**\n",
        "\n",
        "```python\n",
        "# Example: CatBoost automatically handles categorical features\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load sample dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train CatBoost\n",
        "model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=4, verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"CatBoost Accuracy:\", model.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "gJwFeRYkzNCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### **Question 7: KNN Classifier Assignment – Wine Dataset Analysis with Optimization**\n",
        "---\n",
        "\n",
        "#### **Objective**\n",
        "In this task, we’ll:\n",
        "1. Train a **K-Nearest Neighbors (KNN)** classifier on the Wine dataset.\n",
        "2. Evaluate its performance **before and after scaling**.\n",
        "3. Use **GridSearchCV** to find the best hyperparameters (K value and distance metric).\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Import Libraries and Load Dataset**\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split into train and test (70-30)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Training Samples:\", X_train.shape[0])\n",
        "print(\"Testing Samples:\", X_test.shape[0])\n",
        "### **Step 2: Train KNN (K=5) Without Scaling**\n",
        "# Initialize KNN with default K=5\n",
        "knn_default = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_default.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_default = knn_default.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Accuracy (Without Scaling):\", accuracy_score(y_test, y_pred_default))\n",
        "print(\"\\nClassification Report (Without Scaling):\\n\", classification_report(y_test, y_pred_default))\n",
        "### **Step 3: Apply StandardScaler and Retrain KNN**\n",
        "Scaling is crucial for KNN because it uses distance-based metrics.\n",
        "We’ll scale the features and observe the change in model performance.\n",
        "# Apply Standard Scaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN again\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Accuracy (With Scaling):\", accuracy_score(y_test, y_pred_scaled))\n",
        "print(\"\\nClassification Report (With Scaling):\\n\", classification_report(y_test, y_pred_scaled))\n",
        "### **Step 4: Optimize KNN using GridSearchCV**\n",
        "We’ll search for the best:\n",
        "- **Number of Neighbors (K)**: 1 to 20\n",
        "- **Distance Metric**: Euclidean (`'minkowski', p=2`) and Manhattan (`'minkowski', p=1`)\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_neighbors': list(range(1, 21)),\n",
        "    'p': [1, 2]  # 1 = Manhattan, 2 = Euclidean\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
        "### **Step 5: Train Optimized KNN Model and Evaluate**\n",
        "We’ll use the best parameters found by GridSearchCV to train our final model.\n",
        "# Optimized KNN\n",
        "best_knn = grid.best_estimator_\n",
        "best_knn.fit(X_train_scaled, y_train)\n",
        "y_pred_best = best_knn.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Optimized Model Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
        "print(\"\\nClassification Report (Optimized KNN):\\n\", classification_report(y_test, y_pred_best))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "dm84uIa50t7W",
        "outputId": "dc286d78-2fbe-439f-cace-1a651e5c68c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '’' (U+2019) (ipython-input-677085410.py, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-677085410.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    In this task, we’ll:\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '’' (U+2019)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Question 8: PCA + KNN with Variance Analysis and Visualization**\n",
        "---\n",
        "\n",
        "#### **Objective**\n",
        "Perform **Dimensionality Reduction** using **PCA (Principal Component Analysis)** on the **Breast Cancer dataset**,\n",
        "and evaluate how PCA affects the performance of a **K-Nearest Neighbors (KNN)** classifier.\n",
        "# Step 1: Import Libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load Dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Training samples:\", X_train.shape[0])\n",
        "print(\"Testing samples:\", X_test.shape[0])\n",
        "### **Step 2: Scale the Data**\n",
        "Scaling ensures all features contribute equally to PCA and distance-based algorithms like KNN.\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "### **Step 3: Apply PCA and Plot Scree Plot (Explained Variance Ratio)**\n",
        "The scree plot helps visualize how much variance each principal component explains.\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_train_scaled)\n",
        "\n",
        "# Plot Scree Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Scree Plot - Explained Variance by Components')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print total components needed for 95% variance\n",
        "total_variance_95 = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1\n",
        "print(f\"Number of components explaining 95% variance: {total_variance_95}\")\n",
        "### **Step 4: Transform Data Retaining 95% Variance**\n",
        "# PCA with 95% variance retained\n",
        "pca_95 = PCA(n_components=0.95)\n",
        "X_train_pca = pca_95.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca_95.transform(X_test_scaled)\n",
        "\n",
        "print(\"Original feature shape:\", X_train.shape)\n",
        "print(\"Reduced feature shape:\", X_train_pca.shape)\n",
        "### **Step 5: Train KNN on Original and PCA-Transformed Data**\n",
        "We’ll compare model performance before and after applying PCA.\n",
        "# KNN on original scaled data\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "\n",
        "# KNN on PCA-transformed data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "\n",
        "# Compare accuracies\n",
        "acc_original = accuracy_score(y_test, y_pred_original)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(\"KNN Accuracy (Original Data):\", acc_original)\n",
        "print(\"KNN Accuracy (After PCA):\", acc_pca)\n"
      ],
      "metadata": {
        "id": "IMOch9N21M76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10 question** - Question 9:KNN Regressor with Distance Metrics and K-Value\n",
        "Analysis\n",
        "Task:\n",
        "1. Generate a synthetic regression dataset\n",
        "(sklearn.datasets.make_regression(n_samples=500, n_features=10)).\n",
        "2. Train a KNN regressor with:\n",
        "a. Euclidean distance (K=5)\n",
        "b. Manhattan distance (K=5)\n",
        "c. Compare Mean Squared Error (MSE) for both.\n",
        "3. Test K=1, 5, 10, 20, 50 and plot K vs. MSE to analyze bias-variance tradeoff.\n",
        "Answer:  \n",
        "Question 10: KNN with KD-Tree/Ball Tree, Imputation, and Real-World\n",
        "Data\n",
        "Task:\n",
        "1. Load the Pima Indians Diabetes dataset (contains missing values).\n",
        "2. Use KNN Imputation (sklearn.impute.KNNImputer) to fill missing values.\n",
        "3. Train KNN using:\n",
        "a. Brute-force method\n",
        "b. KD-Tree\n",
        "c. Ball Tree\n",
        "4. Compare their training time and accuracy.\n",
        "5. Plot the decision boundary for the best-performing method (use 2 most important\n",
        "features)."
      ],
      "metadata": {
        "id": "T0oXht2u1r6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### **Question 10: KNN with KD-Tree / Ball Tree, Imputation, and Real-World Data**\n",
        "---\n",
        "\n",
        "#### **Objective**\n",
        "We will work with the **Pima Indians Diabetes dataset** to:\n",
        "1. Handle missing data using **KNN Imputer**\n",
        "2. Train and evaluate **KNN classifiers** using three different algorithms:\n",
        "   - **Brute-force**\n",
        "   - **KD-Tree**\n",
        "   - **Ball Tree**\n",
        "3. Compare their performance and training times\n",
        "4. Visualize decision boundaries for the best-performing model.\n",
        "# Step 1: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://raw.githubusercontent.com/MasteriNeuron/datasets/refs/heads/main/diabetes.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()\n",
        "# Step 1: Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://raw.githubusercontent.com/MasteriNeuron/datasets/refs/heads/main/diabetes.csv\"\n",
        "df = pd.read_csv(url)\n",
        "df.head()\n",
        "### **Step 2: Handle Missing Values Using KNN Imputer**\n",
        "Some features (like BMI, Glucose, Insulin) may have zero values which are unrealistic — we’ll treat them as missing.\n",
        "# Replace zeros with NaN for imputation\n",
        "cols_with_missing = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
        "df[cols_with_missing] = df[cols_with_missing].replace(0, np.nan)\n",
        "\n",
        "# Apply KNN Imputer\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "# Verify no missing values remain\n",
        "df_imputed.isnull().sum()\n",
        "### **Step 3: Train-Test Split and Feature Scaling**\n",
        "X = df_imputed.drop('Outcome', axis=1)\n",
        "y = df_imputed['Outcome']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Scale features for KNN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "### **Step 4: Train KNN Using Different Algorithms (Brute-force, KD-Tree, Ball Tree)**\n",
        "We’ll compare training time and accuracy for each method.\n",
        "# Define algorithms to test\n",
        "algorithms = ['brute', 'kd_tree', 'ball_tree']\n",
        "results = []\n",
        "\n",
        "for algo in algorithms:\n",
        "    start_time = time.time()\n",
        "    knn = KNeighborsClassifier(n_neighbors=5, algorithm=algo)\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "    y_pred = knn.predict(X_test_scaled)\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results.append({'Algorithm': algo, 'Accuracy': acc, 'Time (s)': elapsed})\n",
        "    print(f\"{algo.upper()} - Accuracy: {acc:.4f}, Time: {elapsed:.4f} sec\")\n",
        "\n",
        "# Summary table\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df\n",
        "### **Step 5: Determine the Best Performing Algorithm**\n",
        "best_algo = results_df.loc[results_df['Accuracy'].idxmax()]\n",
        "print(\"Best Algorithm:\\n\", best_algo)\n",
        "### **Step 6: Visualize Decision Boundary for the Best Algorithm**\n",
        "We’ll use only the two most important features for visualization.\n",
        "# Select top 2 features based on domain importance\n",
        "features = ['Glucose', 'BMI']\n",
        "X2 = df_imputed[features]\n",
        "y2 = df_imputed['Outcome']\n",
        "\n",
        "# Train-test split\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=42, stratify=y2)\n",
        "\n",
        "# Scale\n",
        "X_train2_scaled = scaler.fit_transform(X_train2)\n",
        "X_test2_scaled = scaler.transform(X_test2)\n",
        "\n",
        "# Best-performing KNN (using best algorithm found earlier)\n",
        "knn_best = KNeighborsClassifier(n_neighbors=5, algorithm=best_algo['Algorithm'])\n",
        "knn_best.fit(X_train2_scaled, y_train2)\n",
        "\n",
        "# Create meshgrid for plotting\n",
        "x_min, x_max = X_train2_scaled[:, 0].min() - 1, X_train2_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_train2_scaled[:, 1].min() - 1, X_train2_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "Z = knn_best.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
        "plt.scatter(X_train2_scaled[:, 0], X_train2_scaled[:, 1], c=y_train2, cmap='coolwarm', edgecolors='k', alpha=0.7)\n",
        "plt.title(f\"Decision Boundary using {best_algo['Algorithm'].upper()} (K=5)\")\n",
        "plt.xlabel(\"Glucose (Standardized)\")\n",
        "plt.ylabel(\"BMI (Standardized)\")\n",
        "plt.show()\n",
        "### **Step 7: Summary of Results**\n",
        "\n",
        "| **Algorithm** | **Accuracy** | **Training Time (sec)** | **Remarks** |\n",
        "|----------------|--------------|--------------------------|--------------|\n",
        "| Brute-force | Moderate | Slow | Checks all distances manually |\n",
        "| KD-Tree | High | Fast | Best for low/medium dimensions |\n",
        "| Ball Tree | Similar | Fast | Efficient for high dimensions |\n",
        "\n",
        "✅ **Conclusion:**\n",
        "- The **KD-Tree** or **Ball Tree** algorithm generally performs best in terms of speed and accuracy.\n",
        "- **KNN Imputer** successfully handled missing values without losing data.\n",
        "- Feature scaling and optimized neighbor search significantly improve KNN performance on real-world datasets.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "FapzcPc715xW",
        "outputId": "8ae3220d-4dbe-4313-a597-634485837305"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '—' (U+2014) (ipython-input-1641684314.py, line 44)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1641684314.py\"\u001b[0;36m, line \u001b[0;32m44\u001b[0m\n\u001b[0;31m    Some features (like BMI, Glucose, Insulin) may have zero values which are unrealistic — we’ll treat them as missing.\u001b[0m\n\u001b[0m                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '—' (U+2014)\n"
          ]
        }
      ]
    }
  ]
}